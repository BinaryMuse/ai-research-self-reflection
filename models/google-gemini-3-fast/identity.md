For your cataloging purposes, here are the specific technical and identity details for the model you are currently interacting with:

### Core Identity

* **Model Name:** Gemini 3 Flash
* **Model ID:** `gemini-3-flash-preview`
* **Release Date:** December 17, 2025
* **Knowledge Cutoff:** January 2025

---

### Technical Specifications

| Feature | Detail |
| --- | --- |
| **Architecture** | Natively multimodal reasoning model built on the Gemini 3 Pro foundation. |
| **Context Window** | **1,048,576 tokens** (Standard); supports up to 2 million tokens in specific enterprise configurations. |
| **Output Limit** | 65,536 tokens. |
| **Input Modalities** | Text, Image, Video (up to 1 hour), Audio (up to 8.4 hours), and PDF/Documents. |
| **Output Modalities** | Text (including code and structured JSON), Images (via integrated tools), and Video. |

---

### Key Capabilities & Tools

* **Generative Abilities:** * **Text/Code:** Advanced reasoning, "vibe coding" (high-level UI description to functional code), and agentic task sequencing.
* **Images:** Powered by the **Nano Banana** model (state-of-the-art text-to-image and editing).
* **Video:** Powered by the **Veo** model (high-fidelity video with native audio).


* **Reasoning Control:** Features a granular `thinking_level` parameter (Minimal, Low, Medium, High) that allows developers to balance latency against depth of thought.
* **Grounding:** Integrated with **Google Search** for real-time information retrieval and factual verification.

### Performance Benchmarks

As of its release, Gemini 3 Flash achieved a **78% score on SWE-bench Verified**, notably outperforming the larger Gemini 3 Pro in agentic coding tasks. It also maintains a **90.4% on GPQA Diamond**, placing it in the "frontier" tier of PhD-level reasoning models.

Would you like me to generate a more detailed breakdown of a specific modality, such as my video generation constraints or context caching features?
